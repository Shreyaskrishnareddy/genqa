# ğŸ““ Jupyter Notebook: gen_qa_rev_openai.ipynb

**Notebook Overview:**
- Total Cells: 29
- Code Cells: 15
- Markdown Cells: 14

**Kernel:** ml

**Language:** python 3.9.6

## ğŸ“‹ Notebook Content:

### ğŸ“ Markdown Cell 1

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/docs/gen-qa-openai.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/pinecone-io/examples/blob/master/docs/gen-qa-openai.ipynb)

---

### ğŸ“ Markdown Cell 2

# Retrieval Enhanced Generative Question Answering with OpenAI

#### Fixing LLMs that Hallucinate

In this notebook we will learn how to query relevant contexts to our queries from Pinecone, and pass these to a generative OpenAI model to generate an answer backed by real data sources. Required installs for this notebook are:

---

### ğŸ”¹ Code Cell 3 [Execution: 1]

```python
!pip install -qU \
    openai \
    pinecone-client \
    pinecone-datasets \
    tqdm \
    pinecone-notebooks
```

**Output:**
```
[2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m327.4/327.4 kB[0m [31m3.8 MB/s[0m eta [36m0:00:00[0m
[2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m216.2/216.2 kB[0m [31m6.0 MB/s[0m eta [36m0:00:00[0m
[2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m75.6/75.6 kB[0m [31m4.1 MB/s[0m eta [36m0:00:00[0m
[2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m215.9/215.9 kB[0m [31m5.5 MB/s[0m eta [36m0:00:00[0m
[2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m34.9/34.9 MB[0m [31m12.9 MB/s[0m eta [36m0:00:00[0m
[2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m3.1/3.1 MB[0m [31m21.1 MB/s[0m eta [36m0:00:00[0m
[2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m77.9/77.9 kB[0m [31m4.5 MB/s[0m eta [36m0:00:00[0m
[2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m58.3/58.3 kB[0m [31m3.9 MB/s[0m eta [36m0:00:00[0m
[2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m76.6/76.6 kB[0m [31m6.1 MB/s[0m eta [36m0:00:00[0m
[2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m73.5/73.5 kB[0m [31m6.6 MB/s[0m eta [36m0:00:00[0m
[2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m73.4/73.4 kB[0m [31m6.9 MB/s[0m eta [36m0:00:00[0m
[2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m11.1/11.1 MB[0m [31m58.3 MB/s[0m eta [36m0:00:00[0m
[2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m143.9/143.9 kB[0m [31m13.4 MB/s[0m eta [36m0:00:00[0m
[?25h[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 11.0.0 which is incompatible.[0m[31m
[0m
```

---

### ğŸ“ Markdown Cell 4

---

## Building a Knowledge Base

Building more reliable LLMs tools requires an external _"Knowledge Base"_, a place where we can store and use to efficiently retrieve information. We can think of this as the external _long-term memory_ of our LLM.

We will need to retrieve information that is semantically related to our queries, to do this we need to use _"dense vector embeddings"_. These can be thought of as numerical representations of the *meaning* behind our sentences.

There are many options for creating these dense vectors, like open source [sentence transformers](https://pinecone.io/learn/nlp/) or OpenAI's [ada-002 model](https://youtu.be/ocxq84ocYi0). We will use OpenAI's offering in this example.

We have already precomputed the embeddings here to speed things up. If you'd like to work through the full process however, check out [this notebook](https://github.com/pinecone-io/examples/blob/master/learn/generation/openai/gen-qa-openai.ipynb).

To download our precomputed embeddings we use Pinecone datasets:

---

### ğŸ”¹ Code Cell 5 [Execution: 5]

```python
from pinecone_datasets import load_dataset

dataset = load_dataset('youtube-transcripts-text-embedding-ada-002')
# we drop sparse_values as they are not needed for this example
dataset.documents.drop(['metadata'], axis=1, inplace=True)
dataset.documents.rename(columns={'blob': 'metadata'}, inplace=True)
ds=dataset.head(5000)
```

---

### ğŸ“ Markdown Cell 6

Now we need a place to store these embeddings and enable a efficient _vector search_ through them all. To do that we use Pinecone.

---

### ğŸ“ Markdown Cell 7

## Creating an Index

Now the data is ready, we can set up our index to store it.

We begin by initializing our connection to Pinecone. To do this we need a [free API key](https://app.pinecone.io).

---

### ğŸ”¹ Code Cell 8 [Execution: 6]

```python
import os
from google.colab import userdata
api_key=userdata.get('pinecone-key')
```

---

### ğŸ”¹ Code Cell 9 [Execution: 7]

```python
from pinecone import Pinecone



# configure client
pc = Pinecone(api_key=api_key)
```

---

### ğŸ“ Markdown Cell 10

Now we setup our index specification, this allows us to define the cloud provider and region where we want to deploy our index. You can find a list of all [available providers and regions here](https://docs.pinecone.io/docs/projects).

---

### ğŸ”¹ Code Cell 11 [Execution: 8]

```python
from pinecone import ServerlessSpec

cloud = os.environ.get('PINECONE_CLOUD') or 'aws'
region = os.environ.get('PINECONE_REGION') or 'us-east-1'

spec = ServerlessSpec(cloud=cloud, region=region)
```

---

### ğŸ”¹ Code Cell 12 [Execution: 10]

```python
index_name = 'gen-qa-openai-fast'
if index_name not in pc.list_indexes().names():
  pc.delete_index(index_name)
```

---

### ğŸ”¹ Code Cell 13 [Execution: 11]

```python
# check if index already exists (it shouldn't if this is first time)
if index_name not in pc.list_indexes().names():
    # if does not exist, create index
    pc.create_index(
        index_name,
        dimension=1536,  # dimensionality of text-embedding-ada-002
        metric='cosine',
        spec=spec
    )
# connect to index
index = pc.Index(index_name)
# view index stats
index.describe_index_stats()
```

**Output:**
```
{'dimension': 1536,
 'index_fullness': 0.0,
 'namespaces': {},
 'total_vector_count': 0}
```

---

### ğŸ“ Markdown Cell 14

We can see the index is currently empty with a `total_vector_count` of `0`. We can begin populating it with OpenAI `text-embedding-ada-002` built embeddings like so:

---

### ğŸ”¹ Code Cell 15 [Execution: 13]

```python
for batch in dataset.iter_documents(batch_size=100):
    index.upsert(batch)
```

---

### ğŸ“ Markdown Cell 16

Now we've added all of our langchain docs to the index. With that we can move on to retrieval and then answer generation.

---

### ğŸ“ Markdown Cell 17

## Retrieval

---

### ğŸ“ Markdown Cell 18

To search through our documents we first need to create a query vector `xq`. Using `xq` we will retrieve the most relevant chunks from the LangChain docs. To create that query vector we must initialize a `text-embedding-ada-002` embedding model with OpenAI. For this, you need an [OpenAI API key](https://platform.openai.com/).

---

### ğŸ”¹ Code Cell 19 [Execution: 19]

```python
import openai
openai.api_key = userdata.get("openai-key")
from openai import OpenAI
client = OpenAI(api_key=openai.api_key)



embed_model = "text-embedding-ada-002"
```

---

### ğŸ”¹ Code Cell 20 [Execution: 23]

```python
query = (
    "Which training method should I use for sentence transformers when " +
    "I only have pairs of related sentences?"
)

res = client.embeddings.create(
    input=[query],
    model=embed_model
)

# retrieve from Pinecone
xq = res.data[0].embedding

# get relevant contexts (including the questions)
res = index.query(vector=xq, top_k=2, include_metadata=True)
```

---

### ğŸ”¹ Code Cell 21

```python
res
```

**Output:**
```
{'matches': [{'id': 'pNvujJ1XyeQ-t418.88',
              'metadata': {'channel_id': 'UCv83tO5cePwHMt1952IVVHw',
                           'end': 568.0,
                           'published': '2021-11-24 16:24:24 UTC',
                           'start': 418.0,
                           'text': 'pairs of related sentences you can go '
                                   'ahead and actually try training or '
                                   'fine-tuning using NLI with multiple '
                                   "negative ranking loss. If you don't have "
                                   'that fine. Another option is that you have '
                                   'a semantic textual similarity data set or '
                                   'STS and what this is is you have so you '
                                   'have sentence A here, sentence B here and '
                                   'then you have a score from from 0 to 1 '
                                   'that tells you the similarity between '
                                   'those two scores and you would train this '
                                   'using something like cosine similarity '
                                   "loss. Now if that's not an option and your "
                                   'focus or use case is on building a '
                                   'sentence transformer for another language '
                                   'where there is no current sentence '
                                   'transformer you can use multilingual '
                                   'parallel data. So what I mean by that is '
                                   'so parallel data just means translation '
                                   'pairs so if you have for example a English '
                                   'sentence and then you have another '
                                   'language here so it can it can be anything '
                                   "I'm just going to put XX and that XX is "
                                   'your target language you can fine-tune a '
                                   'model using something called multilingual '
                                   'knowledge distillation and what that does '
                                   'is takes a monolingual model for example '
                                   'in English and using those translation '
                                   'pairs it distills the knowledge the '
                                   'semantic similarity knowledge from that '
                                   'monolingual English model into a '
                                   'multilingual model which can handle both '
                                   'English and your target language. So '
                                   "they're three options quite popular very "
                                   'common that you can go for and as a '
                                   'supervised methods the chances are that '
                                   'probably going to outperform anything you '
                                   'do with unsupervised training at least for '
                                   'now. So if none of those sound like '
                                   'something',
                           'title': 'Today Unsupervised Sentence Transformers, '
                                    'Tomorrow Skynet (how TSDAE works)',
                           'url': 'https://youtu.be/pNvujJ1XyeQ'},
              'score': 0.865291,
              'values': []},
             {'id': 'WS1uVMGhlWQ-t747.92',
              'metadata': {'channel_id': 'UCv83tO5cePwHMt1952IVVHw',
                           'end': 906.0,
                           'published': '2021-10-20 17:06:20 UTC',
                           'start': 747.0,
                           'text': "pooling approach. Or we can't use it in "
                                   'its current form. Now the solution to this '
                                   'problem was introduced by two people in '
                                   '2019 Nils Reimers and Irenia Gurevich. '
                                   'They introduced what is the first sentence '
                                   'transformer or sentence BERT. And it was '
                                   'found that sentence BERT or S BERT '
                                   'outformed all of the previous Save the Art '
                                   'models on pretty much all benchmarks. Not '
                                   'all of them but most of them. And it did '
                                   'it in a very quick time. So if we compare '
                                   'it to BERT, if we wanted to find the most '
                                   'similar sentence pair from 10,000 '
                                   'sentences in that 2019 paper they found '
                                   'that with BERT that took 65 hours. With S '
                                   'BERT embeddings they could create all the '
                                   'embeddings in just around five seconds. '
                                   'And then they could compare all those with '
                                   "cosine similarity in 0.01 seconds. So it's "
                                   'a lot faster. We go from 65 hours to just '
                                   'over five seconds which is I think pretty '
                                   "incredible. Now I think that's pretty much "
                                   'all the context we need behind sentence '
                                   'transformers. And what we do now is dive '
                                   'into a little bit of how they actually '
                                   'work. Now we said before we have the core '
                                   'transform models and what S BERT does is '
                                   'fine tunes on sentence pairs using what is '
                                   'called a Siamese architecture or Siamese '
                                   'network. What we mean by a Siamese network '
                                   'is that we have what we can see, what can '
                                   'view as two BERT models that are identical '
                                   'and the weights between those two models '
                                   'are tied. Now in reality when implementing '
                                   'this we just use a single BERT model. And '
                                   'what we do is we process one sentence, a '
                                   'sentence A through the model and then we '
                                   'process another sentence, sentence B '
                                   "through the model. And that's the sentence "
                                   'pair. So with our cross-linked we were '
                                   'processing the sentence pair together. We '
                                   'were putting them both together, '
                                   'processing them all at once. This time we '
                                   'process them separately. And during '
                                   'training what happens is the weights '
                                   'within BERT are optimized to reduce the '
                                   'difference between two vector embeddings '
                                   'or two sentence',
                           'title': 'Intro to Sentence Embeddings with '
                                    'Transformers',
                           'url': 'https://youtu.be/WS1uVMGhlWQ'},
              'score': 0.86448735,
              'values': []}],
 'namespace': '',
 'usage': {'read_units': 6}}
```

---

### ğŸ“ Markdown Cell 22

We write some functions to handle the retrieval and completion steps:

---

### ğŸ”¹ Code Cell 23 [Execution: 24]

```python
limit = 3750

import time

def retrieve(query):
    res = client.embeddings.create(
        input=[query],
        model=embed_model
    )

    # retrieve from Pinecone
    xq = res.data[0].embedding

    # get relevant contexts
    contexts = []
    time_waited = 0
    while (len(contexts) < 3 and time_waited < 60 * 12):
        res = index.query(vector=xq, top_k=3, include_metadata=True)
        contexts = contexts + [
            x['metadata']['text'] for x in res['matches']
        ]
        print(f"Retrieved {len(contexts)} contexts, sleeping for 15 seconds...")
        time.sleep(15)
        time_waited += 15

    if time_waited >= 60 * 12:
        print("Timed out waiting for contexts to be retrieved.")
        contexts = ["No contexts retrieved. Try to answer the question yourself!"]


    # build our prompt with the retrieved contexts included
    prompt_start = (
        "Answer the question based on the context below.\n\n"+
        "Context:\n"
    )
    prompt_end = (
        f"\n\nQuestion: {query}\nAnswer:"
    )
    # append contexts until hitting limit
    for i in range(1, len(contexts)):
        if len("\n\n---\n\n".join(contexts[:i])) >= limit:
            prompt = (
                prompt_start +
                "\n\n---\n\n".join(contexts[:i-1]) +
                prompt_end
            )
            break
        elif i == len(contexts)-1:
            prompt = (
                prompt_start +
                "\n\n---\n\n".join(contexts) +
                prompt_end
            )
    return prompt


def complete(prompt):
    # instructions
    sys_prompt = "You are a helpful assistant that always answers questions."
    # query text-davinci-003
    res = client.chat.completions.create(
        model='gpt-3.5-turbo',
        messages=[
            {"role": "system", "content": sys_prompt},
            {"role": "user", "content": prompt}
        ],
        temperature=0
    )
    return res.choices[0].message
```

---

### ğŸ”¹ Code Cell 24 [Execution: 25]

```python
# first we retrieve relevant items from Pinecone
query_with_contexts = retrieve(query)
query_with_contexts
```

**Output:**
```
Retrieved 3 contexts, sleeping for 15 seconds...
"Answer the question based on the context below.\n\nContext:\npooling approach. Or we can't use it in its current form. Now the solution to this problem was introduced by two people in 2019 Nils Reimers and Irenia Gurevich. They introduced what is the first sentence transformer or sentence BERT. And it was found that sentence BERT or S BERT outformed all of the previous Save the Art models on pretty much all benchmarks. Not all of them but most of them. And it did it in a very quick time. So if we compare it to BERT, if we wanted to find the most similar sentence pair from 10,000 sentences in that 2019 paper they found that with BERT that took 65 hours. With S BERT embeddings they could create all the embeddings in just around five seconds. And then they could compare all those with cosine similarity in 0.01 seconds. So it's a lot faster. We go from 65 hours to just over five seconds which is I think pretty incredible. Now I think that's pretty much all the context we need behind sentence transformers. And what we do now is dive into a little bit of how they actually work. Now we said before we have the core transform models and what S BERT does is fine tunes on sentence pairs using what is called a Siamese architecture or Siamese network. What we mean by a Siamese network is that we have what we can see, what can view as two BERT models that are identical and the weights between those two models are tied. Now in reality when implementing this we just use a single BERT model. And what we do is we process one sentence, a sentence A through the model and then we process another sentence, sentence B through the model. And that's the sentence pair. So with our cross-linked we were processing the sentence pair together. We were putting them both together, processing them all at once. This time we process them separately. And during training what happens is the weights within BERT are optimized to reduce the difference between two vector embeddings or two sentence\n\n---\n\npairs of related sentences you can go ahead and actually try training or fine-tuning using NLI with multiple negative ranking loss. If you don't have that fine. Another option is that you have a semantic textual similarity data set or STS and what this is is you have so you have sentence A here, sentence B here and then you have a score from from 0 to 1 that tells you the similarity between those two scores and you would train this using something like cosine similarity loss. Now if that's not an option and your focus or use case is on building a sentence transformer for another language where there is no current sentence transformer you can use multilingual parallel data. So what I mean by that is so parallel data just means translation pairs so if you have for example a English sentence and then you have another language here so it can it can be anything I'm just going to put XX and that XX is your target language you can fine-tune a model using something called multilingual knowledge distillation and what that does is takes a monolingual model for example in English and using those translation pairs it distills the knowledge the semantic similarity knowledge from that monolingual English model into a multilingual model which can handle both English and your target language. So they're three options quite popular very common that you can go for and as a supervised methods the chances are that probably going to outperform anything you do with unsupervised training at least for now. So if none of those sound like something\n\n---\n\nWe're going to be focusing on one specific training method which I think is quite useful because all it really needs is a reasonably small data set of parallel data which is simply translation pairs from a source language like English to whichever other language you're using. So obviously if you are wanting to train a sentence transformer in a language that doesn't really have that much data, it's particularly sentence similarity data, this can be really useful for actually taking a high performing, for example, English sentence transformer and transferring that knowledge or distilling that knowledge into a sentence transformer for your own language. So I think this will be pretty useful for a lot of you. And let's jump straight into it. Before we really get into the whole multilingual sentence transformer part of the video, I just want to sort of give an impression of what these multilingual sentence transformers are actually doing. So on here we can see a single English sentence or brief phrase down at the bottom, I love plants, and the rest of these are all in Italian. So what we have here are a vector representations of dense vector representations of these phrases. And a monolingual sentence transformer, which is most of the sentence transformers, will only cope with one language. So we would hope that phrases that have a similar meaning end up within the same sort of vector space. So like we have for amo lippiante here, and I love plants, these are kind of in the same space. A monolingual sentence transformer would do that for similar sentences. So in English, we might have I love plants and I like plants, which is actually what we have up here. So this here is Italian for I like plants. And we would hope that they're in a similar area, whereas irrelevant or\n\nQuestion: Which training method should I use for sentence transformers when I only have pairs of related sentences?\nAnswer:"
```

---

### ğŸ”¹ Code Cell 25 [Execution: 26]

```python
# then we complete the context-infused query
complete(query_with_contexts)
```

**Output:**
```
ChatCompletionMessage(content='When you only have pairs of related sentences for training sentence transformers, you can use the NLI (Natural Language Inference) with multiple negative ranking loss method. This method involves training or fine-tuning the model using pairs of related sentences and optimizing the weights within the model to reduce the difference between the vector embeddings of the sentence pairs.', role='assistant', function_call=None, tool_calls=None)
```

---

### ğŸ“ Markdown Cell 26

And we get a pretty great answer straight away, specifying to use _multiple-rankings loss_ (also called _multiple negatives ranking loss_).

---

### ğŸ“ Markdown Cell 27

Once we're done with the index we delete it to save resources:

---

### ğŸ”¹ Code Cell 28 [Execution: 27]

```python
pc.delete_index(index_name)
```

**Output:**
```
WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))': /indexes
```

---

### ğŸ“ Markdown Cell 29

---

---
