"""
Python code extracted from gen_qa_rev_openai.ipynb
Generated by Project Publisher

Original notebook: 29 cells
Code cells: 15
"""

# Imports
from google.colab import userdata
from openai import OpenAI
from pinecone import Pinecone
from pinecone import ServerlessSpec
from pinecone_datasets import load_dataset
import openai
import os
import time

# Main code
!pip install -qU \
    openai \
    pinecone-client \
    pinecone-datasets \
    tqdm \
    pinecone-notebooks
dataset = load_dataset('youtube-transcripts-text-embedding-ada-002')
# we drop sparse_values as they are not needed for this example
dataset.documents.drop(['metadata'], axis=1, inplace=True)
dataset.documents.rename(columns={'blob': 'metadata'}, inplace=True)
ds=dataset.head(5000)
api_key=userdata.get('pinecone-key')
# configure client
pc = Pinecone(api_key=api_key)
cloud = os.environ.get('PINECONE_CLOUD') or 'aws'
region = os.environ.get('PINECONE_REGION') or 'us-east-1'
spec = ServerlessSpec(cloud=cloud, region=region)
index_name = 'gen-qa-openai-fast'
if index_name not in pc.list_indexes().names():
  pc.delete_index(index_name)
# check if index already exists (it shouldn't if this is first time)
if index_name not in pc.list_indexes().names():
    # if does not exist, create index
    pc.create_index(
        index_name,
        dimension=1536,  # dimensionality of text-embedding-ada-002
        metric='cosine',
        spec=spec
    )
# connect to index
index = pc.Index(index_name)
# view index stats
index.describe_index_stats()
for batch in dataset.iter_documents(batch_size=100):
    index.upsert(batch)
openai.api_key = userdata.get("openai-key")
client = OpenAI(api_key=openai.api_key)
embed_model = "text-embedding-ada-002"
query = (
    "Which training method should I use for sentence transformers when " +
    "I only have pairs of related sentences?"
)
res = client.embeddings.create(
    input=[query],
    model=embed_model
)
# retrieve from Pinecone
xq = res.data[0].embedding
# get relevant contexts (including the questions)
res = index.query(vector=xq, top_k=2, include_metadata=True)
res
limit = 3750
def retrieve(query):
    res = client.embeddings.create(
        input=[query],
        model=embed_model
    )
    # retrieve from Pinecone
    xq = res.data[0].embedding
    # get relevant contexts
    contexts = []
    time_waited = 0
    while (len(contexts) < 3 and time_waited < 60 * 12):
        res = index.query(vector=xq, top_k=3, include_metadata=True)
        contexts = contexts + [
            x['metadata']['text'] for x in res['matches']
        ]
        print(f"Retrieved {len(contexts)} contexts, sleeping for 15 seconds...")
        time.sleep(15)
        time_waited += 15
    if time_waited >= 60 * 12:
        print("Timed out waiting for contexts to be retrieved.")
        contexts = ["No contexts retrieved. Try to answer the question yourself!"]
    # build our prompt with the retrieved contexts included
    prompt_start = (
        "Answer the question based on the context below.\n\n"+
        "Context:\n"
    )
    prompt_end = (
        f"\n\nQuestion: {query}\nAnswer:"
    )
    # append contexts until hitting limit
    for i in range(1, len(contexts)):
        if len("\n\n---\n\n".join(contexts[:i])) >= limit:
            prompt = (
                prompt_start +
                "\n\n---\n\n".join(contexts[:i-1]) +
                prompt_end
            )
            break
        elif i == len(contexts)-1:
            prompt = (
                prompt_start +
                "\n\n---\n\n".join(contexts) +
                prompt_end
            )
    return prompt
def complete(prompt):
    # instructions
    sys_prompt = "You are a helpful assistant that always answers questions."
    # query text-davinci-003
    res = client.chat.completions.create(
        model='gpt-3.5-turbo',
        messages=[
            {"role": "system", "content": sys_prompt},
            {"role": "user", "content": prompt}
        ],
        temperature=0
    )
    return res.choices[0].message
# first we retrieve relevant items from Pinecone
query_with_contexts = retrieve(query)
query_with_contexts
# then we complete the context-infused query
complete(query_with_contexts)
pc.delete_index(index_name)

if __name__ == "__main__":
    # Run the main functionality
    print("Notebook code extracted successfully")